"""
src/scrapers/product_link_scraper.py

Hepsiburada'dan Ã¼rÃ¼n linklerini toplayan scraper.
- Kategorileri tarar
- Yorum sayÄ±sÄ± 1000+ olan Ã¼rÃ¼nleri filtre eder
- DeÄŸerlendirme Ã¶zeti olan Ã¼rÃ¼nleri seÃ§er
- SonuÃ§larÄ± .txt dosyasÄ±na kaydeder
"""

import logging
from typing import List, Set, Optional
from pathlib import Path
from dataclasses import dataclass

from selenium.webdriver.common.by import By
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import TimeoutException, NoSuchElementException

from src.scrapers.base_scraper import BaseScraper, retry_on_failure
from src.config.config_settings import settings


logger = logging.getLogger(__name__)


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Ä°LERLEYÄ°Å TAKIP
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

@dataclass
class ScrapingProgress:
    """Scraping'in nerede olduÄŸunu takip eder."""
    total_products_found: int = 0
    valid_products: int = 0
    categories_processed: int = 0
    pages_scraped: int = 0
    errors: int = 0
    skipped_no_reviews: int = 0
    skipped_no_summary: int = 0

    def __str__(self) -> str:
        return (
            f"Bulunan: {self.valid_products}/{settings.MAX_PRODUCTS} Ã¼rÃ¼n | "
            f"Kategori: {self.categories_processed} | "
            f"Sayfa: {self.pages_scraped} | "
            f"Hata: {self.errors}"
        )


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# PRODUCT LINK SCRAPER
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

class ProductLinkScraper(BaseScraper):
    """
    Hepsiburada'dan Ã¼rÃ¼n URL'lerini toplayan scraper.

    KullanÄ±mÄ±:
        scraper = ProductLinkScraper(headless=True)
        links = scraper.scrape(max_products=100)
    """

    # CSS Selectors â€” Hepsiburada'nÄ±n kullandÄ±ÄŸÄ± selectors
    PRODUCT_CARD_SELECTOR = "li.productListContent-zAP0Y5msy8OHn5z7T_K_"
    REVIEW_COUNT_SELECTOR = "span.rate-module_count__fjUng"
    SUMMARY_HEADING_XPATH = "//h2[contains(text(), 'DeÄŸerlendirme Ã¶zeti')]"

    def __init__(self, **kwargs):
        super().__init__(**kwargs)
        self.progress = ScrapingProgress()
        self.collected_links: Set[str] = set()

    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # ANA METOT
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    def scrape(
        self,
        categories: Optional[List[str]] = None,
        max_products: int = settings.MAX_PRODUCTS,
        min_reviews: int = settings.MIN_REVIEWS_REQUIRED,
        output_file: Optional[Path] = None,
    ) -> Set[str]:
        """
        Kategorileri tarar ve Ã¼rÃ¼n linklerini toplur.

        Args:
            categories: TarayacaÄŸÄ±nÄ±z kategori URL'leri (None ise settings'den alÄ±r)
            max_products: KaÃ§ Ã¼rÃ¼n toplayacaÄŸÄ±nÄ±z
            min_reviews: Minimum yorum sayÄ±sÄ±
            output_file: SonuÃ§larÄ±n kaydedileceÄŸi dosya yolu

        Returns:
            Toplanan Ã¼rÃ¼n URL'lerinin seti
        """
        if categories is None:
            categories = settings.HEPSIBURADA_CATEGORIES

        if output_file is None:
            output_file = settings.RAW_DATA_DIR / settings.PRODUCT_LINKS_FILE

        logger.info(
            f"Scraping baÅŸladÄ± â†’ {len(categories)} kategori | "
            f"Hedef: {max_products} Ã¼rÃ¼n | Min yorum: {min_reviews}"
        )

        try:
            for category_url in categories:
                # Hedef Ã¼rÃ¼n sayÄ±sÄ±na ulaÅŸtÄ±ysanÄ±z durun
                if len(self.collected_links) >= max_products:
                    logger.info(f"Hedef {max_products} Ã¼rÃ¼ne ulaÅŸtÄ±!")
                    break

                self._scrape_category(
                    category_url=category_url,
                    max_products=max_products,
                    min_reviews=min_reviews,
                )
                self.progress.categories_processed += 1

            # SonuÃ§larÄ± kaydet
            self._save_links(output_file)
            logger.info(f"TamamlandÄ±. {self.progress}")
            return self.collected_links

        except Exception as e:
            logger.error(f"Scraping baÅŸarÄ±sÄ±z: {e}", exc_info=True)
            raise
        finally:
            self.close()

    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # BIR KATEGORÄ° TARAMA
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    def _scrape_category(
        self,
        category_url: str,
        max_products: int,
        min_reviews: int,
    ) -> None:
        """Bir kategori iÃ§indeki sayfalarÄ± tarar."""
        logger.info(f"Kategori baÅŸladÄ±: {category_url}")

        for page_num in range(1, 51):  # Max 50 sayfa
            if len(self.collected_links) >= max_products:
                break

            try:
                page_url = f"{category_url}?sayfa={page_num}"
                self._scrape_page(page_url, min_reviews)
                self.progress.pages_scraped += 1

            except Exception as e:
                logger.warning(f"Sayfa {page_num} hatasÄ±: {e}")
                self.progress.errors += 1
                continue

    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # BIR SAYFA TARAMA
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    @retry_on_failure(max_retries=3)
    def _scrape_page(self, page_url: str, min_reviews: int) -> None:
        """Tek bir sayfayÄ± tarar, Ã¼rÃ¼n kartlarÄ±nÄ± inceler."""
        self.get_page(page_url)

        # ÃœrÃ¼n kartlarÄ± yÃ¼klenene kadar bekle
        try:
            self.wait.until(
                EC.presence_of_element_located(
                    (By.CSS_SELECTOR, self.PRODUCT_CARD_SELECTOR)
                )
            )
        except TimeoutException:
            logger.warning(f"ÃœrÃ¼n bulunamadÄ±: {page_url}")
            return

        product_cards = self.driver.find_elements(
            By.CSS_SELECTOR, self.PRODUCT_CARD_SELECTOR
        )
        logger.debug(f"{len(product_cards)} Ã¼rÃ¼n kartÄ± bulundu")

        # â”€â”€ Yorum sayÄ±sÄ±na gÃ¶re filtre â”€â”€
        potential_links: List[str] = []

        for card in product_cards:
            try:
                review_text = self.safe_get_text(
                    By.CSS_SELECTOR,
                    self.REVIEW_COUNT_SELECTOR,
                    parent=card,
                )

                if not review_text:
                    continue

                # "(1.234)" â†’ 1234
                review_count = int(
                    review_text.strip("()").replace(".", "").replace(",", "")
                )

                if review_count >= min_reviews:
                    link_el = card.find_element(By.TAG_NAME, "a")
                    url = link_el.get_attribute("href")

                    if url and url not in self.collected_links:
                        potential_links.append(url)
                        self.progress.total_products_found += 1
                else:
                    self.progress.skipped_no_reviews += 1

            except (NoSuchElementException, ValueError):
                continue

        # â”€â”€ DeÄŸerlendirme Ã¶zeti kontrolÃ¼ â”€â”€
        for product_url in potential_links:
            if len(self.collected_links) >= settings.MAX_PRODUCTS:
                break

            if self._has_review_summary(product_url):
                self.collected_links.add(product_url)
                self.progress.valid_products += 1
                logger.info(
                    f"âœ… Eklendi ({self.progress.valid_products}/"
                    f"{settings.MAX_PRODUCTS})"
                )
            else:
                self.progress.skipped_no_summary += 1

    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # Ã–ZET KONTROLÃœ
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    def _has_review_summary(self, product_url: str) -> bool:
        """ÃœrÃ¼nÃ¼n yorum Ã¶zeti var mÄ± kontrol eder."""
        reviews_url = self._get_reviews_url(product_url)

        # Yeni sekme aÃ§
        self.driver.execute_script(f"window.open('{reviews_url}');")
        self.driver.switch_to.window(self.driver.window_handles[-1])

        try:
            self.wait.until(
                EC.presence_of_element_located(
                    (By.XPATH, self.SUMMARY_HEADING_XPATH)
                )
            )
            has_summary = True
        except TimeoutException:
            has_summary = False
        finally:
            # Sekmeyi kapat, ana sekmeye dÃ¶n
            self.driver.close()
            self.driver.switch_to.window(self.driver.window_handles[0])

        return has_summary

    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # YARDIMCI METOTLAR
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    @staticmethod
    def _get_reviews_url(product_url: str) -> str:
        """ÃœrÃ¼n URL'sini yorum sayfasÄ± URL'sine Ã§evirir."""
        clean_url = product_url.split("?")[0]
        if "-yorumlari" not in clean_url:
            clean_url += "-yorumlari"
        return clean_url

    def _save_links(self, output_file: Path) -> None:
        """Toplanan linkler dosyaya kaydedilir."""
        output_file.parent.mkdir(parents=True, exist_ok=True)

        with open(output_file, "w", encoding="utf-8") as f:
            for link in sorted(self.collected_links):
                f.write(f"{link}\n")

        logger.info(f"ğŸ’¾ {len(self.collected_links)} link â†’ {output_file}")


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Ã‡ALIÅTIRMA (python -m src.scrapers.product_link_scraper)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

if __name__ == "__main__":
    import logging

    logging.basicConfig(
        level=logging.INFO,
        format="%(asctime)s [%(levelname)s] %(name)s: %(message)s",
    )

    print("ğŸš€ Product Link Scraper baÅŸlÄ±yor...\n")

    with ProductLinkScraper(headless=True) as scraper:
        links = scraper.scrape(
            max_products=10,   # Test iÃ§in kÃ¼Ã§Ã¼k sayÄ±
            min_reviews=1000,
        )

    print(f"\n{'â”€' * 50}")
    print(f"âœ… Toplanan Ã¼rÃ¼n sayÄ±sÄ±: {len(links)}")
    print(f"ğŸ“ Kaydetilen dosya: {settings.RAW_DATA_DIR / settings.PRODUCT_LINKS_FILE}")